ggplot(results, aes(x=as.factor(Dataset), y=as.numeric(Accuracy),fill=Index))+
geom_bar(stat="identity", position=position_dodge())+
geom_text(aes(label=round(as.numeric(Accuracy),2)), vjust=.18, color="black",
position = position_dodge(0.9), size=2.5,angle = 90) +
facet_grid( ~Dataset , scales="free")+
labs(x = "Set de datos",
y = "Acierto",
fill = "")+
theme(axis.title.x = element_text(color = "black", size = 10, face = "bold"),
axis.text.x = element_text(color = "white"))
library(writexl)
write_xlsx(results, "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/R/Classifier/Results/classification_results_ARIMA_models-Pearson2-SSIMT.xlsx")
library(devtools)
library(githubinstall)
#install_github("juancbellass/time-series-r-package", force = TRUE)
pkgs <- c("ggplot2", "TSdist", "philentropy", "caret", "Metrics", "dplyr",
"parallel", "SimilarityMeasures", "TSSimilarityIndeces")
inst <- lapply(pkgs, library, character.only = TRUE) # Load them
# dtw_ <- function(S1, S2){
#   win <- ceiling(length(S1)/.1)
#   path <- dtw::dtw(S1,S2, #distance.only = TRUE,
#                    window.type="sakoechiba",
#                    window.size=win)#step.pattern =symmetric2)
#   sim <- mean(abs(S1[path$index1] - S2[path$index2]))
#   return(sim)
# }
Pearsons.Measure <- function(s1, s2){return(1-cor(s1, s2))}
index.list <- list(Pearson = Pearsons.Measure,
SSIMT = SSIMT)
# Manhattan =ManhattanDistance, #1
#                 Euclidean = EuclideanDistance, #2
#                 Chebyshev = Chebyshev.Measure, #3
#                 MSE = mse, #4
#                 Frechet = Frechets.Measure, #5
#                 DTW = DTW.Measure, #6
#                 ERP = ERP.Measure, #7
#                 EDR = EDR.Measure, #8
#                 Fourier = FourierDistance, #9
#                 Pearson = Pearsons.Measure, #10
#                 CORT = CORT.Measure, #11
#                 Chouakria = CortDistance, #12
#                 LCSS = LCSS.Measure) #13
index.names <- names(index.list); index.names
# Data set load function
load_data <- function(file_names, size, common_path) {
data_list <- vector('list', length(file_names))
names(data_list) <- file_names
for (name in file_names) {
# Load data
data_ <- read.table(file = paste0(common_path, name, "/", name, "_TEST.tsv"),
sep = "\t", header = FALSE)
# Check if name is FiftyWords
if (name != "FiftyWords") {
# Calcular el tamaño de la clase más pequeña
class_sizes <- table(data_$V1)
smallest_class_size <- min(class_sizes)
if (size > smallest_class_size) {size <- smallest_class_size}
# sampling
muestras_por_clase <- data_ %>%
group_by(V1) %>%
sample_n(size)
# Add into the list
data_list[[name]] <- data.matrix(muestras_por_clase)
} else{data_list[[name]] <- data.matrix(data_)}
}
return(data_list)
}
set.seed(1988)
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/Data_sets/UCR/UCRArchive_2018/"
primary_dirs <- list.files(common_path)
primary_dirs <- sample(primary_dirs, 40) #define the number of datasets
ds_list <- load_data(primary_dirs, 30, common_path)
primary_dirs
#parallel process setup
num_cores <- detectCores() - 1
cl <- makeCluster(num_cores) #number clusters to the parallel process
clusterEvalQ(cl=cl, {library(TSSimilarityIndeces);
library(SimilarityMeasures);
library(TSdist);
library(philentropy);
library(caret);
library(Metrics)})
# Create a process bar
# max = 40 ds x 6 combinaciones de clases x 13 índices
pb <- txtProgressBar(min = 0, max = 40, style = 3)
i <- 0
results <-  data.frame(stringsAsFactors = F)
# Select the model
for (ds in primary_dirs) {
data <- ds_list[[ds]]
clases <- sort(unique(data[,1]))
if (length(clases)>3) {clases <- clases[1:3]}
# Select the first class
for (c1 in 1:length(clases)) {
row_index_1 <- which(data[,1] %in% clases[c1])
DS1 <- data[row_index_1, -1]
# Select the second class
for (c2 in c1:length(clases)) {
row_index_2 <- which(data[,1] %in% clases[c2])
DS2 <- data[row_index_2, -1]
for (indice in index.names) {
FUN <- index.list[[indice]]
# Similarity calculation
for (s in 1:dim(DS1)[1]) {
sim <- parApply(cl, DS2[-s,], 1, FUN, DS1[s,])
dftoappend <- cbind(ds, sim, indice, clases[c1], clases[c2])
colnames(dftoappend) <- c("Dataset", "Value", "Index", "Class_1", "Class_2")
results <<- rbind(results, dftoappend)
}
}
}
}
setTxtProgressBar(pb, i)
i <- i+1
}
# Stop the previously created cluster
stopCluster(cl)
# Stop the previously created cluster
stopCluster(cl)
library("writexl")
write_xlsx(results, "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/R/new_similarity_measure/results/variabilidad_entre_clases_UCR_experimentos-Pearson2-SSIMT.xlsx")
#library(devtools)
#library(githubinstall)
#install_github("juancbellass/time-series-r-package", force = TRUE)
pkgs = c("ggplot2", "TSdist", "philentropy", "caret", "Metrics", "Matrix",
"dplyr", "parallel", "SimilarityMeasures", "TSSimilarityIndeces")
inst = lapply(pkgs, library, character.only = TRUE) # Load them
# dtw_ <- function(S1, S2){
#   win <- ceiling(length(S1)/.1)
#   path <- dtw::dtw(S1,S2, #distance.only = TRUE,
#                    window.type="sakoechiba",
#                    window.size=win)#step.pattern =symmetric2)
#   sim <- mean(abs(S1[path$index1] - S2[path$index2]))
#   return(sim)
# }
Pearsons.Measure <- function(s1, s2){return(1-cor(s1, s2))}
#library(devtools)
#library(githubinstall)
#install_github("juancbellass/time-series-r-package", force = TRUE)
pkgs = c("ggplot2", "TSdist", "philentropy", "caret", "Metrics", "Matrix",
"dplyr", "parallel", "SimilarityMeasures", "TSSimilarityIndeces")
inst = lapply(pkgs, library, character.only = TRUE) # Load them
# dtw_ <- function(S1, S2){
#   win <- ceiling(length(S1)/.1)
#   path <- dtw::dtw(S1,S2, #distance.only = TRUE,
#                    window.type="sakoechiba",
#                    window.size=win)#step.pattern =symmetric2)
#   sim <- mean(abs(S1[path$index1] - S2[path$index2]))
#   return(sim)
# }
Pearsons.Measure <- function(s1, s2){return(1-cor(s1, s2))}
index.list <- list(Pearson = Pearsons.Measure,
SSIMT = SSIMT)
# Manhattan =ManhattanDistance, #1
#                 Euclidean = EuclideanDistance, #2
#                 Chebyshev = Chebyshev.Measure, #3
#                 MSE = mse, #4
#                 Frechet = Frechets.Measure, #5
#                 DTW = DTW.Measure, #6
#                 ERP = ERP.Measure, #7
#                 EDR = EDR.Measure, #8
#                 Fourier = FourierDistance, #9
#                 Pearson = Pearsons.Measure, #10
#                 CORT = CORT.Measure, #11
#                 Chouakria = CortDistance, #12
#                 LCSS = LCSS.Measure) #13
index.names <- names(index.list); index.names
library(openxlsx)
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
names(df.list) <- primary_dirs
df.names <- names(df.list); df.names
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
names(df.list) <- primary_dirs
df.names <- names(df.list); df.names
df.names
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/para_ordenador"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/para_ordenador"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/para_ordenador/"
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/para_ordenador/"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
names(df.list) <- primary_dirs
df.names <- names(df.list); df.names
head(df.list[[1]])
noises <- unique(df.list[[1]]$Noise); noises
ids <- unique(df.list[[1]]$ID)
degrees <- unique(df.list[[1]]$Degree); degrees
classes <- unique(df.list[[1]]$Class); classes
#parallel process setup
num_cores <- detectCores() - 1
cl = makeCluster(num_cores) #number clusters to the parallel process
clusterEvalQ(cl=cl, {library(TSSimilarityIndeces);library(dtw);
library(SimilarityMeasures); library(TSdist);
library(philentropy); library(caret); library(Metrics);
library(Matrix); library(dplyr)})
results <- data.frame(stringsAsFactors = F)
pb <- progress::progress_bar$new(total = length(df.list) * length(index.names) * length(noises) * 50)
for (d in seq_along(df.list)){
df <- df.list[[d]]
unique_vals <- head(unique(df$ID), 50)
df1 <- df[df$ID %in% unique_vals, ]
for(indice in index.names) {
# index selection
FUNCION <- index.list[[indice]]
for (noise in noises) {
# noise selection
df2 <- df1[df1$Noise == noise, ]
for(id in unique_vals) {
#select the original series and its variations
df3 <- data.matrix(df2[df2$ID == id, -(1:5)])
#similarity vector computation
simil.vector <- apply(df3[-1,], 1, FUNCION, df3[1,])
#correct order evaluation
D <- tril(outer(simil.vector, simil.vector, "-"))
c <- sum(D > 0) #c <- length((D[D>0]))
nro <- dim(D)[1]
ordenados <- c/(nro*(nro-1)/2)
toappened <- data.frame(Dataset = names(df.list)[d], Index = indice,
Noise = noise, Score = ordenados)
results <- rbind(results, toappened)
#cat(paste(names(df.list)[d], noise, indice), "\n")
}
pb$tick() #paste(d, noise, indice)
}
}
}
stopCluster(cl)
library(writexl)
write_xlsx(results, "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/R/new_similarity_measure/results/orden_results_UCR_Pearson2-SSIMT.xlsx")
# library(devtools)
# library(githubinstall)
#install_github("juancbellass/time-series-r-package", force = TRUE)
pkgs = c("ggplot2", "TSdist", "philentropy", "caret", "Metrics", "dtw",
"parallel", "SimilarityMeasures", "TSSimilarityIndeces")
inst = lapply(pkgs, library, character.only = TRUE) # Load them
library(openxlsx)
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCR-contaminados/para_clasificador/"
primary_dirs <- list.files(common_path, pattern = "*.csv")
df.test.list <- list()
#nro.series = 30 # number of series per dataset
c <- 1 #counter
for (dir in primary_dirs) {
df.test.list[[c]] <- read.csv(paste(common_path,dir, sep=""), header = TRUE)
c <- c+1
}
# Remove the string "-contaminados.csv" from each name in the list
primary_dirs <- gsub("-contaminado.csv", "", primary_dirs)
names(df.test.list) <- primary_dirs
df.names <- names(df.test.list); df.names
# Data set load function
load_data <- function(file_names, size, common_path, tail_) {
data_list <- vector('list', length(file_names))
names(data_list) <- file_names
for (name in file_names) {
# Load data
data_ <- read.table(file = paste0(common_path, name, "/", name, tail_),
sep = "\t", header = FALSE)
# Get the number of observations of each class
class_count <- table(data_$V1)
# Find the classes and their sizes
classes <- names(class_count)
sizes <- as.vector(class_count)
# Initialize the data frame to store the samples
sample_df <- data.frame(matrix(ncol = ncol(data_), nrow = 0))
colnames(sample_df) <- colnames(data_)
# Take N samples of each class
for (i in 1:length(classes)) {
if (sizes[i] >= size) {
class_sample <- subset(data_, data_[,1] == classes[i])
class_sample <- class_sample[sample(nrow(class_sample), size), ]
} else {
class_sample <- subset(data_, data_[,1] == classes[i])
}
sample_df <- rbind(sample_df, class_sample)
}
# Add into the list
data_list[[name]] <- data.matrix(sample_df)
}
return(data_list)
}
set.seed(1988)
common_path <- "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/data_sets/UCR/UCRArchive_2018/"
primary_dirs <- list.files(common_path)
primary_dirs <- sample(primary_dirs, 40) #define the number of datasets
n_per_class <- 30
df.train.list <- load_data(primary_dirs, n_per_class, common_path, "_TRAIN.tsv")
names(df.train.list) <- primary_dirs
# dtw_ <- function(S1, S2){
#   win <- ceiling(length(S1)/.1)
#   path <- dtw::dtw(S1,S2, #distance.only = TRUE,
#                    window.type="sakoechiba",
#                    window.size=win)#step.pattern =symmetric2)
#   sim <- mean(abs(S1[path$index1] - S2[path$index2]))
#   return(sim)
# }
Pearsons.Measure <- function(s1, s2){return(1-cor(s1, s2))}
index.list <- list(Pearson = Pearsons.Measure,
SSIMT = SSIMT)
# Manhattan =ManhattanDistance, #1
#                 Euclidean = EuclideanDistance, #2
#                 Chebyshev = Chebyshev.Measure, #3
#                 MSE = mse, #4
#                 Frechet = Frechets.Measure, #5
#                 DTW = DTW.Measure, #6
#                 ERP = ERP.Measure, #7
#                 EDR = EDR.Measure, #8
#                 Fourier = FourierDistance, #9
#                 Pearson = Pearsons.Measure, #10
#                 CORT = CORT.Measure, #11
#                 Chouakria = CortDistance, #12
#                 LCSS = LCSS.Measure) #13
index.names <- names(index.list); index.names
nearest_neighbors = function(x, obs, k, FUN, p=NULL){
"
x:    matriz de nxm donde cada fila es una serie de tiempo de referencia. Hay n
series de longitud m.
obs:  matriz de 1Xm que representa a una serie de longitud m la cual queremos
clasificar comparando con las series de referencia.
k:    nro de vecinos más cercanos.
FUN:  medida de distancia.
p:    algún parámetro extra de la función FUN
"
# Checkeamos si el nro de observaciones es igual
if (ncol(x) != length(obs)){stop('Las series deben tener la misma longitud')}
# Calulculamos la distancia, considerando p por Minkowski
if(is.null(p)){
dist = parApply(cl, x, 1, FUN, obs)
}else{
dist = parApply(cl, x, 1, FUN, obs, p)
}
# Encontrar el vecino más cercno
idx = order(dist)[1:k]
distances = dist[idx]
if (length(unique(distances)) != k){
warning(
paste("Varias variables con igual distancia. Se usó k:", length(neighbor_ind))
)
}
ret = list(neighbor_ind = idx, distances = distances) # OJO! las dos listas no son correspondientes!
return(ret)
}
# Predicción del algoritmo KNN
knn_prediction = function(x){
pred = names(which.max(table(x[1])))
return(pred)
}
# Predicción para varias series
knn = function(x_fit, x_pred, k,
func = Euclidean.Index, weighted_pred = F, p = NULL){
"
x_fi:   dataframe con series de referencia (by row) y primera columna como etiqueta de clase
x_pred: dataframe con series a clasificar (by row) y primera columna como etiqueta de clase
k:      nro de vecinos más cercanos
func:   medida de distancia
weighted_pred:  no sé que es!
p:      parámetro extra de la func medida de distancia
"
# Inicializamos las predicciones
predictions <- c()
# Para cada observación, obtenemos la predicción
for (i in 1:nrow(x_pred)){
#print(i)
neighbors <- nearest_neighbors(x_fit[,-1],
x_pred[i,-1], k, FUN = func)
if (weighted_pred){
pred <- knn_prediction(x_fit[neighbors[[1]], ], neighbors[[2]])
}else{
pred <- knn_prediction(x_fit[neighbors[[1]], ])
}
# Si hay más de 1 clase predicha, hacer una prediccion con k más 1
if(length(pred) > 1 && nrow(x_fit) <= k){
pred <- knn(x_fit, t(as.matrix(x_pred[i,])), k=k+1,
func = func, weighted_pred = weighted_pred, p==p)
}
predictions[i] <- pred[1]
}
return(predictions)
}
#parallel process setup
num_cores <- detectCores() - 1
cl = makeCluster(num_cores) #number clusters to the parallel process
clusterEvalQ(cl=cl, {library(TSSimilarityIndeces); library(dtw);
library(SimilarityMeasures); library(TSdist); library(philentropy); library(caret); library(Metrics)})
df.names1 <- primary_dirs[15]
index.names1 <- index.names
noises <- unique(df.test.list[[1]]$Noise)
results <- data.frame(Dataset = character(),
Noise = character(),
Noise_Level = numeric(),
Index = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE)
# Select the Dataset
for (df in df.names1)
{
train <- df.train.list[[df]]
test_0 <- df.test.list[[df]]
# Select the Noise
for (noise in noises)
{
test_1 <- test_0[test_0$Noise == noise, ]
n_levels <- unique(test_1$Degree)
for (n_level in n_levels)
{
# Select the Noise Level
test_2 <- test_1[test_1$Degree == n_level, ]
# Drop the unnecessary columns
test <- data.matrix(subset(test_2, select = -c(DataFrame, Noise, ID, Degree)))
# Select the Measure
for (i in index.names1)
{
funcion <- index.list[[i]]
predicciones <- knn(train, test, k=1, func=funcion)
predicciones <- factor(predicciones, levels=levels(as.factor(test[,1])))
ConfMat <- confusionMatrix(as.factor(predicciones), as.factor(test[,1]))
toappend <- data.frame(Dataset = df,
Noise = noise,
Noise_Level = n_level,
Index = i,
Accuracy = ConfMat$overall[1])
colnames(toappend) <- c("Dataset", "Noise", "Noise_Level", "Index", "Accuracy")
results <- rbind(results, toappend)
print(paste(df, noise, n_level, i, ConfMat$overall[1]))
}
}
}
}
df.names1 <- primary_dirs#[15]
index.names1 <- index.names
noises <- unique(df.test.list[[1]]$Noise)
results <- data.frame(Dataset = character(),
Noise = character(),
Noise_Level = numeric(),
Index = character(),
Accuracy = numeric(),
stringsAsFactors = FALSE)
# Select the Dataset
for (df in df.names1)
{
train <- df.train.list[[df]]
test_0 <- df.test.list[[df]]
# Select the Noise
for (noise in noises)
{
test_1 <- test_0[test_0$Noise == noise, ]
n_levels <- unique(test_1$Degree)
for (n_level in n_levels)
{
# Select the Noise Level
test_2 <- test_1[test_1$Degree == n_level, ]
# Drop the unnecessary columns
test <- data.matrix(subset(test_2, select = -c(DataFrame, Noise, ID, Degree)))
# Select the Measure
for (i in index.names1)
{
funcion <- index.list[[i]]
predicciones <- knn(train, test, k=1, func=funcion)
predicciones <- factor(predicciones, levels=levels(as.factor(test[,1])))
ConfMat <- confusionMatrix(as.factor(predicciones), as.factor(test[,1]))
toappend <- data.frame(Dataset = df,
Noise = noise,
Noise_Level = n_level,
Index = i,
Accuracy = ConfMat$overall[1])
colnames(toappend) <- c("Dataset", "Noise", "Noise_Level", "Index", "Accuracy")
results <- rbind(results, toappend)
print(paste(df, noise, n_level, i, ConfMat$overall[1]))
}
}
}
}
stopCluster(cl)
library("writexl")
write_xlsx(results, "C:/Users/Usuario/Google Drive (juancbellassai@mi.unc.edu.ar)/Juanc Doctorado/R/Niveles_de_contaminacon-Curvas_y_Sensibilidad/Accuracy/results/robustez-Pearson2-SSIMT.xlsx")
